{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZW8b5ycDB9K"
      },
      "source": [
        "# <font face=\"Verdana\" size=6 color='#6495ED'>  KALMAN FILTERS - Hands On\n",
        "\n",
        "<font face=\"Verdana\" size=3 color='#40E0D0'> Profs. Larissa Driemeier, Arturo Forner-Cordero and Thiago Martins\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1fV0ruX4TCznLnNw-3ZV390p2JyU_-llu' width=\"600\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INTRODUCTION: Presentation of the problem\n",
        "\n",
        "Visual information is the most important type of information perceived, processed and interpreted by the human brain. As we know, computers don't have *eyes*, so, how can they still find patterns in an image?\n",
        "\n",
        "Digital image processing is the use of a computer to perform some operations on digital images by means of an algorithm, in order to get enhanced image either to extract some useful information.\n",
        "\n",
        "Before we jump into image processing world, let's define what exactly constitutes an image. An image is nothing more than a two-dimensional matrix (3-D in the case of color images) defined by $f(x,y)$ where $x$ and $y$ represent the dimensions (width and height, respectively) based on the number of pixels. Each pixel is an image point. The value of $f(x,y) $ describes a specific shade, opacity or color of the pixel. It is usually represented as:\n",
        "\n",
        "* Grayscale - $f(x,y)$ is a single number stored as an 8-bit integer with value between 0 to 255, representing the brightness of the pixel (0:  black; 255: white).\n",
        "* RGB - to represent color images, $f(x,y)$ is made up of 3 integers representing the components of red (R), green (G) and blue (B) for each pixel. Thus, “value” of the pixel becomes a vector of three numbers. For example, let's take the YELLOW color. It is formed by combining these three colors where the RED (intensity) value is 255, the BLUE is 255 and the GREEN is 0. Often, the three different components are stored as three separate *grayscale* images known as color planes, which need to be recombined during display or processing.\n",
        "* RGBA - is an extension of RGB with an added alpha variable, varying from 0 to 1, which represents the opacity of the image ( 0:fully transparent; 1:fully opaque).\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1vA-pDXxBOBbCjjRXibLiOgdleBIuXFB9' width=\"500\"></center>\n",
        "\n",
        "* HSV - for hue, saturation, value is an alternative representation of the RGB color model, designed in the 1970s.  HSV is closer to how humans perceive color and it usually appears as a cone or cylinder with these three components: *Hue* is the color portion of the model, expressed as a number from 0 to 360 degrees (Red: 0 - 60 degrees, Yellow: 61 - 120 degrees; Green: 121 - 180 degrees, Cyan: 181 - 240 degrees, Blue 241 - 300 degrees, Magenta: 301 - 360 degrees); *Saturation* describes the amount of gray in a particular color, from 0 to 100% (saturation toward zero introduces more gray and produces a faded effect);  *Value* (or Brightness) acts with saturation and defines the brightness or intensity of the color, from 0 to 100%, where 0 is completely black.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1gJziCnLTJcOfk8aLkO2XRCSdDkGzQFGa' width=\"250\"></center>\n",
        "\n",
        "* CIELAB - *CIE* refers to *Commission internationale de l'éclairage* (International Commission on Illumination), an independent, non-profit organization devoted to worldwide cooperation and the exchange of information on all matters relating to the science and art of light and lighting, colour and vision, photobiology and image technology. *Lab* is composed of a \"L\" representing brightness, ranging from 0-100 - the larger the value of L, the brighter; *a* stands for red and green: the value from negative to positive is the evolution from green to red; *b* stands for blue and yellow:the evolution from negative to positive is the evolution from blue to yellow.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1Jg0AjJ2MP5uoYiw89oDqX49BkCZ9aXiH' width=\"400\"></center>\n",
        "\n",
        "In this way, dealing with the values of $f(x,y)$, computers can process, understand, find patterns, etc... in an image.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5cNei3ZVLo6C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5VNoqbtwjyV"
      },
      "outputs": [],
      "source": [
        "!pip install ffmpeg-python\n",
        "\n",
        "from IPython.display import HTML, Javascript, display\n",
        "from base64 import b64encode\n",
        "\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "import imutils\n",
        "\n",
        "import cv2, sys, numpy, os\n",
        "from time import sleep\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your work today\n",
        "\n",
        "You will use computer vision and Kalman Filter to track a ball down a ramp, as shown in the diagram below."
      ],
      "metadata": {
        "id": "rTlWYmpHnJNO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4AXYhjA183G"
      },
      "source": [
        "## Mount Google Drive to store captured videos\n",
        "\n",
        "The first few lines of code allow you to mount your Google Drive to store captured videos and images.\n",
        "\n",
        "Initially, you will be asked to log into your Google account and confirm permissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMLihG_Cl70Q"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPOOhIVCu0iU"
      },
      "source": [
        "Now you will be able to see your Google Drive folders directly from Google Colab.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1CLTPpD7L5gEzFnmYnKuGAoBQiA2lx8dy' width=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the following folders on your drive:\n",
        "* My Drive > Colab Notebooks > KALMAN_2023 > Video Datasets\n",
        "* My Drive > Colab Notebooks > KALMAN_2023 > Image Datasets"
      ],
      "metadata": {
        "id": "HYS1wgVKS2m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "# Folders to store videos and images\n",
        "video_path = '/content/drive/MyDrive/Colab Notebooks/KALMAN_2024/Video Datasets'\n",
        "image_path = '/content/drive/MyDrive/Colab Notebooks/KALMAN_2024/Image Datasets'\n",
        "os.chdir(\".\")\n",
        "print(\"current dir is: %s\" % (os.getcwd()))\n",
        "\n",
        "if os.path.isdir(video_path):\n",
        "    print(\"Folder exists.\")\n",
        "else:\n",
        "    print(\"Folder does not exist.\")\n",
        "    os.mkdir(video_path)\n",
        "    print(\"Folder created!\")\n",
        "\n",
        "if os.path.isdir(image_path):\n",
        "    print(\"Folder exists.\")\n",
        "else:\n",
        "    print(\"Folder does not exist.\")\n",
        "    os.mkdir(image_path)\n",
        "    print(\"Folder created!\")"
      ],
      "metadata": {
        "id": "F_SVD9QaFCB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subfolders of 'Image Datasets' used to store the images:\n",
        "sub_data = 'Result_NoFilter'\n",
        "sub_data2 = 'Result_Filter'\n",
        "# Folders to store images and videos\n",
        "path_nofilter = os.path.join(image_path, sub_data)\n",
        "path_filter = os.path.join(image_path, sub_data2)\n",
        "path = pathlib.Path(path_nofilter)\n",
        "path.mkdir(parents=True, exist_ok=True)\n",
        "path = pathlib.Path(path_filter)\n",
        "path.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "TtjoEa_ZFuKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font face=\"Verdana\" size=6 color='#6495ED'> Part I: create videos\n",
        "\n",
        "With the ball we provided, you must record two videos:\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=17eufK1U-VRFSsaBS6IGBY2lAZDPmulYM' width=\"900\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "dVAadBEqtvzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. video of the ball going down the ramp, according to the image below. Note that the dimensions are approximate and you do not need them to implement your filter. They serve to guide you in your filming. Your challenge will be to make the computer understand what happened to the ball when it disappears behind the red box! In fact, a great challenge for autonomous vehicles: *if something has disappeared for a few moments from the cameras' field of view, it doesn't mean that it doesn't really exist anymore*.  For visual tracking, the constant velocity motion model is the most widely used process model as it can describe e.g. pedestrian movements very well.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1SUvhDXYuxLATuXirtH9VERC92cXBdZ6u' width=\"800\"></center>\n",
        "\n",
        "2. video of the ball bouncing on the ground (or table) 2 or 3 times or transfer the ball from one hand to another 2 or 3 times.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1CRwNHOeo6jgh5XGzDSLqTE3k96KedqDy' width=\"800\"></center>\n",
        "\n",
        "You should create the videos using a cell phone, taking into account that the camera shakes during filming and cut the beginning and ending parts of the video that are irrelevant and will confuse the filter. Film frames are illustrated in Fig.~\\ref{fig:videoParts}. When the sensor misses the ball, the Kalman filter, through the prediction step, must be able to estimate its path.\n",
        "\n",
        "Use the video available on Moodle as a guide.\n",
        "\n",
        "__Attention__: the video `Ramp_YellowBall.mp4` is only for guide you. You must use your own video!"
      ],
      "metadata": {
        "id": "CXLdkUs-xpu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below returns the number of frames for the video specified as input. In OpenCV, the first step towards reading a video file is to create a `VideoCapture` object. Once the `VideoCapture` object has been created, you can use the `read()` method to *read* frames from the video stream. It returns two values: a Boolean value indicating whether the read operation was successful (`grabbed` in the function) and the actual image data in the form of a NumPy array (`frame`). Moreover, the function `shape` returns the dimensions of a frame (in pixels) and number of channels."
      ],
      "metadata": {
        "id": "lx8093IdT9-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_frames_manual(video):\n",
        "\t# initializes the counter for the number of frames\n",
        "\ttotal = 0\n",
        "\t# loop over video frames\n",
        "\twhile True:\n",
        "\t\t# get the current frame\n",
        "\t\tgrabbed, frame = video.read()\n",
        "\n",
        "\t\t# check if we reached the end of the video\n",
        "\t\tif not grabbed:\n",
        "\t\t\tbreak\n",
        "\t\t# increments the total number of frames\n",
        "\t\ttotal += 1\n",
        "\t\theight, width, channels = frame.shape\n",
        "\t# return the total number of frames in the video file, height, width and #channels of each frame\n",
        "\treturn total,height, width, channels"
      ],
      "metadata": {
        "id": "rEwEA2LQOeyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font face=\"Verdana\" size=5 color='#40E0D0'>  Part I: delivery 1#3\n",
        "\n",
        "* Upload the videos* in MP4 format.\n",
        "* Write and submit a small report* for each video following the example below:\n",
        " * Insert a table with: frame dimension, frames per second, total frames and total time.\n",
        " * Choose three frames and insert the information about the time ($t$).\n",
        "\n",
        "<small>*For both experiments<snall>\n",
        "\n",
        " <center><img src='https://drive.google.com/uc?export=view&id=1V3NVcu0QFN_GkDK6vjUllhtbcFvSl9aP' width=\"800\"></center>"
      ],
      "metadata": {
        "id": "JdzEyukGHpAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font face=\"Verdana\" size=6 color='#6495ED'> Part II: Ball detection\n",
        "\n",
        "Now let's detect the ball by image processing. __Repeat for both videos you recorded.__"
      ],
      "metadata": {
        "id": "9AQH01iwFJWW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dlrW0aV2jLa"
      },
      "source": [
        "### Upload the video\n",
        "\n",
        "Upload the video. It will be used for tracking the ball."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_name = 'Ramp_YellowBall.mp4'\n",
        "video_file = os.path.join(video_path , video_name)"
      ],
      "metadata": {
        "id": "HJXdidArT3WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uat7tjre2mgZ"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(video_file):\n",
        "  print('Video available on drive.')\n",
        "  video = cv2.VideoCapture(video_file)\n",
        "  fps = video.get(cv2.CAP_PROP_FPS)\n",
        "  total,height, width, channels = count_frames_manual(video)\n",
        "  print (\"Frames per second   : {:10.4f}\".format(fps))\n",
        "  print (\"Total frames        : {:d}\".format(total))\n",
        "  print (\"Total time, in  [s] : {:10.4f}\".format(total/fps))\n",
        "  print (\"Dimension of a frame: {:d} x {:d}\".format(height, width))\n",
        "  video.release()\n",
        "  cv2.destroyAllWindows()\n",
        "else:\n",
        "  print('Video is not available.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mp4 = open(video_file,'rb').read()\n",
        "decoded_vid = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f'<video width=400 controls><source src={decoded_vid} type=\"video/mp4\"></video>')"
      ],
      "metadata": {
        "id": "q9LzY5LXzrNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssvh_ZWq88qt"
      },
      "source": [
        "### Ball tracking creation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detection margins\n",
        "\n",
        "# Black\n",
        "HSV_lower_black = np.array([0,0,0], dtype=np.uint8)\n",
        "HSV_upper_black = np.array([170,150,50], dtype=np.uint8)\n",
        "\n",
        "# Red\n",
        "HSV_lower_red = np.array([161, 155, 84])\n",
        "HSV_upper_red = np.array([179, 255, 255])\n",
        "\n",
        "# Blue\n",
        "HSV_lower_blue = np.array([94, 80, 2])\n",
        "HSV_upper_blue = np.array([126, 255, 255])\n",
        "\n",
        "# Green\n",
        "HSV_lower_green = np.array([25, 52, 72])\n",
        "HSV_upper_green = np.array([102, 255, 255])\n",
        "\n",
        "# Yellow\n",
        "HSV_lower_yellow = np.array([22, 93, 0], dtype=\"uint8\")\n",
        "HSV_upper_yellow = np.array([45, 255, 255], dtype=\"uint8\")\n",
        "\n",
        "# Any color except white\n",
        "HSV_lower = np.array([0, 42, 0])\n",
        "HSV_upper = np.array([179, 255, 255])"
      ],
      "metadata": {
        "id": "Svin7xB7BUmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_ball(frame, HSV_lower, HSV_upper, Flag_Measurement=False):\n",
        "    \"\"\"\n",
        "     Generates a circle around the ball, identifying its position in the frame\n",
        "     Args:\n",
        "         frame (np.ndarray[float])        : Frame image array\n",
        "         HSV_lower (np.ndarray[int])      : Array with the lower range with HSV data (Hue, Saturation, Value)\n",
        "                                            to create a mask according to the color of the ball.\n",
        "         HSV_upper (np.ndarray[int])      : Array with the upper range with HSV data  (Hue, Saturation, Value)\n",
        "                                            to create a mask according to the color of the ball.\n",
        "         Flag_Measurement (bool, optional): Indicates whether the ball was detected. Default is False.\n",
        "     Returns:\n",
        "         Tuple[float, float, float, bool] : Returns the position of the center of the ball (center[0], center[1]),\n",
        "                                            radius of the ball (radius) and the flag (Flag_Measurement).\n",
        "    \"\"\"\n",
        "    x, y, radius = -1, -1, -1\n",
        "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "#\n",
        "    mask = cv2.inRange(hsv_frame, HSV_lower, HSV_upper)\n",
        "    # uncomment if you cannot understand what the command `cv2.inRange` does\n",
        "    # Be warned that the image will be shown for each frame of the video.\n",
        "    # cv2_imshow(mask)\n",
        "\n",
        "    blur = cv2.medianBlur(mask, 11)\n",
        "    # uncomment if you cannot understand what the command `cv2.medianBlur` does\n",
        "    # Be warned that the image will be shown for each frame of the video.\n",
        "    #cv2_imshow(blur)\n",
        "\n",
        "    circles = cv2.HoughCircles(blur, cv2.HOUGH_GRADIENT, 1, 100, param1=500, param2=10, minRadius=10, maxRadius=100)\n",
        "    # try to understand the minimum necessary about `cv2.HoughCircles` parameters with the OpenCV documentation\n",
        "    center = (-1,-1)\n",
        "    radius = 0.\n",
        "#\n",
        "    if circles is not None:\n",
        "        Flag_Measurement = True\n",
        "        center = (circles[0][0][0],circles[0][0][1])\n",
        "        radius = circles[0][0][2]\n",
        "    return center[0], center[1], radius, Flag_Measurement"
      ],
      "metadata": {
        "id": "jKGrKI8OVHwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir(path_nofilter):\n",
        "  os.mkdir(path_nofilter)\n",
        "\n",
        "# Video\n",
        "webcam = cv2.VideoCapture(video_file)\n",
        "(_, frame) = webcam.read()\n",
        "cv2_imshow(frame)"
      ],
      "metadata": {
        "id": "6WgNV1gDB18M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final video creation and playback\n",
        "\n",
        "After you run the code below, all frames of the film with the ball tracking are saved as `.png` in the folder,\n",
        "\n",
        "My Drive > Colab Notebooks > KALMAN_2023 > Image Datasets > Result_NoFilter\n",
        "\n",
        "The command used for this purpose is\n",
        "```\n",
        "cv2.imwrite('% s/% s.png' % (path, count), frame)\n",
        "```\n",
        "\n",
        "That is, each frame of the movie with the the ball contour and its center added as superscript is saved in the folder defined as `path`."
      ],
      "metadata": {
        "id": "AVnSAAZK_Ett"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = total\n",
        "\n",
        "count = 0\n",
        "while count < N-1:\n",
        "    if not webcam.isOpened():\n",
        "        print('Unable to open the video.')\n",
        "        sleep(5)\n",
        "        pass\n",
        "    (_, frame) = webcam.read()\n",
        "#\n",
        "    # Detect Ball in the frame\n",
        "    [x,y,radius, Flag] = detect_ball(frame, HSV_lower_yellow, HSV_upper_yellow)\n",
        "    z = [x,y]\n",
        "    #\n",
        "    if radius > 10:\n",
        "       #\n",
        "       cv2.circle(frame, (int(x), int(y)), int(radius), (102, 102, 0), 2)\n",
        "       #\n",
        "       cv2.circle(frame, (int(z[0]), int(z[1])), 3, (153, 153, 0), -1)\n",
        "\n",
        "       if (count != 0):\n",
        "         cv2.putText(frame, 'Track', (int(z[0]), int(z[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (16,128,4), 2)\n",
        "         cv2.putText(frame, 'Center coordinates x: '+str(int(z[0]))+' y: '+str(int(z[1])), (120,460), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128,33,4), 2)\n",
        "\n",
        "    cv2.imwrite('% s/% s.png' % (path_nofilter, count), frame)\n",
        "    count += 1\n",
        "webcam.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "UaScTwlyXFoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfaOErqB9DBo"
      },
      "source": [
        "Now, you will turn those images into a video and save it with `BallTrackingResult_NoFilter.mp4`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = [img for img in os.listdir(path_nofilter) if img.endswith(\".png\")]\n",
        "\n",
        "frame = cv2.imread(os.path.join(path_nofilter, images[0]))\n",
        "height, width, layers = frame.shape\n",
        "\n",
        "video_name_result = 'BallTrackingResult_NoFilter.mp4'\n",
        "video_path_result = os.path.join(video_path , video_name_result)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "video = cv2.VideoWriter(video_path_result, fourcc, fps/3, (width,height))\n",
        "\n",
        "for image in images:\n",
        "    frame = cv2.imread(os.path.join(path_nofilter, image))\n",
        "    video.write(frame)\n",
        "\n",
        "video.release()\n",
        "print('Video generated successfully.')"
      ],
      "metadata": {
        "id": "wzXNSJyfmx_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Watch the video in the drive."
      ],
      "metadata": {
        "id": "cXxLfLFlRGHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font face=\"Verdana\" size=5 color='#40E0D0'> Part II: delivery 2#3\n",
        "\n",
        "* The objective is to evaluate your understanding about the ball detection code. Summarize the method used in the `detect_ball` function. For example, explain the commands `cv2.inRange`, `cv2.medianBlur`and `cv2.HoughCircles`. What is the output of each function? Then understand how `detect_ball` function is called. Flowchart the ball detection algorithm.\n",
        "* Submit both videos with the tracking (via computational vision) you just did.\n",
        "* What are your conclusions about the videos? __Attention: do not ask the professor's opinion, summarize the whole group discussion and highlight the most important points .__"
      ],
      "metadata": {
        "id": "KPcRAVX_ZEk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, have fun with Kalman..."
      ],
      "metadata": {
        "id": "etqXhg5447S-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font face=\"Verdana\" size=6 color='#6495ED'> Part III: Kalman Filter\n",
        "\n",
        "The summary of the Kalman filter formulation presented in the Theory class Notebook is repeated here.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{Predict Step}\\\\\n",
        "\\mathbf{\\bar x} &= \\mathbf{F x} + \\mathbf{B u} \\\\\n",
        "\\mathbf{\\bar P} &= \\mathbf{FP{F}}^\\mathsf T + \\mathbf Q \\\\\n",
        "\\\\\n",
        "\\text{Update Step}\\\\\n",
        "\\textbf{S} &= \\mathbf{H\\bar PH}^\\mathsf T + \\mathbf R \\\\\n",
        "\\mathbf K &= \\mathbf{\\bar PH}^\\mathsf T \\mathbf{S}^{-1} \\\\\n",
        "\\textbf{y} &= \\mathbf z - \\mathbf{H \\bar x} \\\\\n",
        "\\hat{\\mathbf x} &=\\mathbf{\\bar x} +\\mathbf{K\\textbf{y}} \\\\\n",
        "\\hat{\\mathbf P}  &= (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar P}(\\mathbf I - \\mathbf{KH})^\\mathsf T + \\mathbf{KRK}^\\mathsf T\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "There is a simplification for the covariance matrix defined in the theoretical class:\n",
        "$$\\hat{\\mathbf P}  = (\\mathbf{I}-\\mathbf{KH})\\mathbf{\\bar P}$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1WZDts3zKTr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### System definition\n",
        "\n",
        "Now your goal is to get the formulation for $\\mathbf F_t, \\mathbf Q_t, \\mathbf H$ and $\\mathbf R_t$.\n",
        "\n",
        "In this problem, the system state vector given by: \\begin{equation*}\n",
        "  \\mathbf x_t = \\begin{bmatrix} x \\\\ y \\\\ v_x \\\\ v_y \\end{bmatrix}\n",
        " \\end{equation*}\n",
        "\n",
        "where $x, y$ are the coordinates of the ball's position, $v_x, v_y$ are the respective velocity components.\n",
        "\n",
        "Attention, __acceleration is not a state quantity__. However, the assumption of perfect constant speed is unrealistic, especially for real-world applications. A more sophisticated model of acceleration can be created, based on the angle of the ramp, rolling friction and the aerodynamic drag of the ball. The difficulty with this approach is that, in addition to demanding the characterization of model parameters, it produces an acceleration in the global reference. The transformation from the global reference to the camera's reference is unknown. __Therefore, we will use the constant velocity model, which is the most used in these cases.__ A relaxation to this strong assumption is allowed by introducing a piecewise constant *white acceleration*. Therefore, a  recurrence law of evolution of the state vector can be written as follows:\n",
        "\n",
        "\\begin{equation}\n",
        " \\mathbf x_{t+1} = \\mathbf F\\mathbf x_{t}+ \\mathbf B \\mathbf u_{t+1} + \\mathbf w_{t+1}\n",
        "\\end{equation}\n",
        "where ${\\mathbf{w}_t \\sim N(\\mathbf 0,\\mathbf Q_t)}$ is a Gaussian random vector with zero mean and covariance matrix $\\mathbf Q_t$, which sums to the deterministic model. Disregarding $\\mathbf B \\mathbf u_{t}$ given that the proposed problem does not include control, one can make explicit eq. (1),\n",
        "\\begin{align}\n",
        "  x_{t+1} &= x_{t}+ \\delta t \\, \\dot{x}_{t}+\\frac{\\delta t^2}{2}\\omega_{x,t}\\\\\n",
        "  y_{t+1} &= y_{t}+ \\delta t \\, \\dot{y}_{t}+\\frac{\\delta t^2}{2}\\omega_{y,t}\\\\\n",
        "  \\dot{x}_{t+1} &= \\dot{x}_{t}+ \\delta t \\, \\omega_{x,t}\\\\\n",
        "  \\dot{y}_{t+1} &= \\dot{y}_{t}+ \\delta t \\, \\omega_{y,t}\\\\\n",
        "\\end{align}\n",
        "where the time step $\\delta t$ is defined as the time between measurements, $\\omega_{x,t},\\omega_{y,t}$ are the process noise corresponding to $x, y$, respectively. These are basically zero-mean Gaussian white noise. In matrix form, we have:\n",
        "$$\n",
        "     \\begin{bmatrix}\n",
        "         x_{t+1} \\\\\n",
        "         y_{t+1} \\\\\n",
        "         \\dot{x}_{t+1} \\\\\n",
        "         \\dot{y}_{t+1}  \n",
        "     \\end{bmatrix}\n",
        "     =\n",
        "     \\begin{bmatrix}\n",
        "         1 & 0 & \\delta t & 0 \\\\\n",
        "         0 & 1 & 0 & \\delta t \\\\\n",
        "         0 & 0 & 1 & 0 \\\\\n",
        "         0 & 0 & 0 & 1  \n",
        "     \\end{bmatrix}\n",
        "      \\times\n",
        "     \\begin{bmatrix}\n",
        "         x_{t} \\\\\n",
        "         y_{t} \\\\\n",
        "         \\dot{x}_{t} \\\\\n",
        "         \\dot{y}_{t}  \n",
        "     \\end{bmatrix}+ \\begin{bmatrix}\n",
        "         \\frac{\\delta t^2}{2}\\omega_{x,t} \\\\\n",
        "         \\frac{\\delta t^2}{2}\\omega_{y,t} \\\\\n",
        "         \\delta t\\,\\omega_{x,t} \\\\\n",
        "         \\delta t \\, \\omega_{y,t}  \n",
        "     \\end{bmatrix}\n",
        "  $$\n",
        "$$\n",
        "\\mathbf Q_{t} =\n",
        "     \\begin{bmatrix}\n",
        "          \\frac{\\delta t^4}{4}\\sigma^2_{\\omega_x} & 0 & \\frac{\\delta t^3}{2}\\sigma^2_{\\omega_x} & 0 \\\\\n",
        "         0 &  \\frac{\\delta t^4}{4}\\sigma^2_{\\omega_y} & 0 & \\frac{\\delta t^3}{2}\\sigma^2_{\\omega_y} \\\\\n",
        "         \\frac{\\delta t^3}{2}\\sigma^2_{\\omega_x} & 0 & \\delta t^2 \\sigma^2_{\\omega_x}& 0 \\\\\n",
        "         0 & \\frac{\\delta t^3}{2}\\sigma^2_{\\omega_y} & 0 & \\delta t^2 \\sigma^2_{\\omega_y}  \n",
        "     \\end{bmatrix}\n",
        "$$\n",
        "To derive the equation of $\\mathbf Q_{t}$, two ideas are important:\n",
        "* $\\mathbb{E}\\left[ \\omega_{x,t} \\omega_{x,t}\\right] = \\sigma^2_{\\omega_x}$ and $\\mathbb{E}\\left[ \\omega_{ y,t} \\omega_{y,t}\\right] = \\sigma^2_{\\omega_y}$, where $\\sigma^2_{\\omega_x},\\sigma^2_{\\omega_y}$ are the variances.\n",
        "* $\\mathbb{E}\\left[ \\omega_{x,t} \\omega_{y,t}\\right] = 0$ because there is no correlation between the $x$ axis and the $y$ axis.\n",
        "\n",
        "Measurements are represented by the vector:\n",
        "\\begin{equation*}\n",
        " z_t = \\begin{bmatrix} c_x \\\\ c_y \\end{bmatrix}\n",
        "\\end{equation*}\n",
        "where $c_x,c_y$ are the coordinates of the center of the sphere (see what the `detect_ball` function returns) and will be considered our *measurements* here. These measurements show independent Gaussian random noise with zero-mean and covariances $r_x^2$ and $r_y^2$, for $x$ and $y$ respectively.\n",
        "\n",
        "Write the system measurement law in the following form:\n",
        "\\begin{equation}\n",
        " \\mathbf z_{t+1} = \\mathbf H \\mathbf {\\bar x}_t + \\mathbf v_{t}\n",
        "\\end{equation}\n",
        "where ${\\mathbf{v}_t \\sim N(\\mathbf 0,\\mathbf R_t)}$ is a Gaussian random vector with zero mean and covariance matrix $\\mathbf R_t$.\n",
        "$$\n",
        "\\mathbf R_{t} =\\mathbb{E}\\left[ \\begin{bmatrix}\n",
        "         v_{x,t} \\\\\n",
        "         v_{y,t}\n",
        "     \\end{bmatrix} \\begin{bmatrix}\n",
        "         v_{x,t} &  v_{y,t}\n",
        "     \\end{bmatrix} \\right]\n",
        "  $$\n",
        "such that:\n",
        "\\begin{equation}\n",
        " \\mathbf R_t = \\begin{bmatrix} r_x^2 & 0 \\\\ 0 & r_y^2  \\end{bmatrix}\n",
        "\\end{equation}\n",
        "if:\n",
        "* $\\mathbb{E}\\left[ v_{x,t} v_{x,t}\\right] = r^2_x$ and $\\mathbb{E}\\left[ v_{y,t} vp_{y,t}\\right] = r^2_y$, where $r^2_x,r^2_y$ are variances.\n",
        "* $\\mathbb{E}\\left[p_{x,t} p_{y,t}\\right] = 0$ because there is no correlation between the measurements of the $x$ and $y$ axes.\n",
        "\n",
        "It is also important to note that, in general, variances $\\sigma^2_{\\omega_x} = \\sigma^2_{\\omega_y}$ and $r^2_x=r^2_y$ are adopted for simplification. This is what we will do here. However, it may be important to adjust them accordingly. __Always think carefully about the parameters you will use in a project.__\n"
      ],
      "metadata": {
        "id": "GSpzwSVkK9B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kalman Filter Algorithm\n",
        "\n",
        "Here you will generate your \"Kalman Filter\" class. The functions are:\n",
        "* `Predict`, which will predict with your model and process;\n",
        "* `Update`, which will make the correction, from the camera measurement;\n",
        "* `ellipseP`, will draw the ellipse (with 2 standard deviations) referring to the covariance `P`. Substitute de circle with the ellipse while you're tracking the ball.\n",
        "\n",
        "See that, in the video, the ellipse with 2 standard deviations is drawn (in red) over the ball.\n",
        "\n",
        "\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=17dTAAiHUkPbMRMVU3EBjOA1Cv8ayHjWv' width=\"400\"></center>\n"
      ],
      "metadata": {
        "id": "X0kCGMSWKTr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KalmanFilter(object):\n",
        "    def __init__(self, dt, u_x,u_y, var_w, varz_x, varz_y, initial_var_pos, initial_var_vel):\n",
        "        \"\"\"\n",
        "          :param dt: time step (time for 1 cycle)\n",
        "          :param u_x: acceleration in the x direction\n",
        "          :param u_y: acceleration in the y direction\n",
        "          :param var_w: process noise variance\n",
        "          :param varz_x: measurement variance in the x direction\n",
        "          :param varz_y: measurement variance in the y direction\n",
        "        \"\"\"\n",
        "        # Period\n",
        "        self.dt = dt\n",
        "        # Control variables\n",
        "        self.u = np.array([[u_x],[u_y]])\n",
        "        # Initial state\n",
        "        self.x = np.array([[500], [300], [0], [0]])\n",
        "        # State Transition Matrix F\n",
        "        self.F = np.array()\n",
        "        # Control Input Matrix B\n",
        "        self.B = np.array()\n",
        "        # Measurement Mapping Matrix\n",
        "        self.H = np.array()\n",
        "        # Process Noise Covariance Q\n",
        "        self.Q = np.array()\n",
        "        # Measurement Noise Covariance R\n",
        "        self.R = np.array()\n",
        "        # Initial Covariance Matrix P\n",
        "        self.P = np.array()\n",
        "\n",
        "    def initial(self,cx,cy):\n",
        "        self.x =[cx,cy,0,0]\n",
        "        return\n",
        "\n",
        "    def predict(self):\n",
        "        self.x =\n",
        "        self.P =\n",
        "        return self.x[0:2]\n",
        "\n",
        "    def update(self, cx,cy):\n",
        "        z = np.array([[cx],\n",
        "                     [cy]])\n",
        "        S =\n",
        "        K =\n",
        "        self.x =\n",
        "        self.P =\n",
        "        return self.x[0:2]\n",
        "\n",
        "    def ellipseP(self,nstd = 2):\n",
        "      #nstd is the #standard deviations\n",
        "      cov = [[self.P[0,0],self.P[0,1]],[self.P[1,0],self.P[1,1]]]\n",
        "      vals, vecs = np.linalg.eigh(cov)\n",
        "      theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
        "    # width, height are diameters, not radius\n",
        "      width, height = 2 * nstd * np.sqrt(vals)\n",
        "      return width, height, theta"
      ],
      "metadata": {
        "id": "kNoKv6Mw25-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Response\n",
        "class KalmanFilter(object):\n",
        "    def __init__(self, dt, u_x,u_y, var_w, varz_x, varz_y, initial_var_pos, initial_var_vel):\n",
        "        \"\"\"\n",
        "          :param dt: time step (time for 1 cycle)\n",
        "          :param u_x: acceleration in the x direction\n",
        "          :param u_y: acceleration in the y direction\n",
        "          :param var_w: process noise variance\n",
        "          :param varz_x: measurement variance in the x direction\n",
        "          :param varz_y: measurement variance in the y direction\n",
        "        \"\"\"\n",
        "        # Period\n",
        "        self.dt = dt\n",
        "        # Control variables\n",
        "        self.u = np.array([[u_x],[u_y]])\n",
        "        # Initial state\n",
        "        self.x = np.array([[500], [300], [0], [0]])\n",
        "        # State Transition Matrix F\n",
        "        self.F = np.array([[1, 0, self.dt, 0],\n",
        "                            [0, 1, 0, self.dt],\n",
        "                            [0, 0, 1, 0],\n",
        "                            [0, 0, 0, 1]])\n",
        "        # Control Input Matrix B\n",
        "        self.B = np.array([[(self.dt**2)/2, 0],\n",
        "                            [0, (self.dt**2)/2],\n",
        "                            [self.dt,0],\n",
        "                            [0,self.dt]])\n",
        "        # Measurement Mapping Matrix\n",
        "        self.H = np.array([[1, 0, 0, 0],\n",
        "                            [0, 1, 0, 0]])\n",
        "        # Process Noise Covariance Q\n",
        "        self.Q = np.array([[(self.dt**4)/4, 0, (self.dt**3)/2, 0],\n",
        "                            [0, (self.dt**4)/4, 0, (self.dt**3)/2],\n",
        "                            [(self.dt**3)/2, 0, self.dt**2, 0],\n",
        "                            [0, (self.dt**3)/2, 0, self.dt**2]]) * var_w\n",
        "        # Measurement Noise Covariance R\n",
        "        self.R = np.array([[varz_x,0],\n",
        "                           [0, varz_y]])\n",
        "        # Initial Covariance Matrix P\n",
        "        self.P = np.array([[initial_var_pos, 0., 0., 0.],\n",
        "                           [0., initial_var_pos, 0., 0.],\n",
        "                           [0., 0., initial_var_vel, 0.],\n",
        "                           [0., 0., 0., initial_var_vel]])\n",
        "\n",
        "    def predict(self):\n",
        "        self.x = self.F.dot(self.x) #+ self.B.dot(self.u)\n",
        "        self.P = self.F.dot(self.P).dot(self.F.T) + self.Q\n",
        "        return self.x[0:2]\n",
        "\n",
        "    def update(self, cx,cy):\n",
        "        z = np.array([[cx],\n",
        "                     [cy]])\n",
        "        S = self.H.dot(self.P).dot(self.H.T) + self.R\n",
        "        K = np.array(self.P.dot(self.H.T).dot(np.linalg.inv(S)))\n",
        "        self.x = self.x + K.dot(z - self.H.dot(self.x))\n",
        "        A = np.eye(self.H.shape[1]) - K.dot(self.H)\n",
        "        self.P = A.dot(self.P).dot(A.T)+K.dot(self.R).dot(K.T)\n",
        "        return self.x[0:2]\n",
        "\n",
        "    def ellipseP(self,nstd = 2):\n",
        "      #nstd is the #standard deviations\n",
        "      cov = [[self.P[0,0],self.P[0,1]],[self.P[1,0],self.P[1,1]]]\n",
        "      vals, vecs = np.linalg.eigh(cov)\n",
        "      theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
        "    # width, height are diameters, not radius\n",
        "      width, height = 2 * nstd * np.sqrt(vals)\n",
        "      return width, height, theta"
      ],
      "metadata": {
        "id": "2qYHq9AHKTr_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameters\n",
        "Here, the parameters are already defined for you. Find an explanation for their values.\n",
        "\n",
        "If you do not agree, you can change. But, remember that you will be alone, sailing through uncharted waters. Ok, you will be engineers and it's good to start having these feelings...\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1Rjoq7CKl6EE46Ec0xef13ko5n7WGilMX' width=\"250\"></center>\n",
        "\n",
        "__Some tips__\n",
        "\n",
        "The dimension of a frame is 480 (H) x 848 (W) and the ramp that is $\\approx 1m$ wide and occupies $\\approx 540$ pixels in the picture.\n",
        "\n",
        "The tennis ball diameter is $\\approx 0.067m$, which means $\\approx 36$ pixels. The *measurement error* is assumed to be around$1/3-1/4$ of the radius, that is, $\\approx \\pm 5$ pixels.\n",
        "\n",
        "For process variance, remember that space unity is pixel, acceleration $g = 9.8 m/s^2$ should be $pixels/s^2$. Moreover, remember that $\\sigma^2_{a}=\\mathbb{E}[\\omega\\omega^T]$ and $\\omega$ is somewhere from $\\frac{1}{6} g$ to $g$.\n",
        "\n",
        " __REMEMBER__ Prior to initiating the Kalman filter, it is essential to obtain measurements from the sensor. This step is considered good practice as it allows for the updating of the initial state estimate, enhancing the accuracy and reliability of the filter's performance.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1lpo5Z9MNK9mwMrmVxkslbbEC9sK8shrz' width=\"1400\"></center>\n"
      ],
      "metadata": {
        "id": "htMJYrEtxcHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## KALMAN FILTER\n",
        "# Period(delta t)\n",
        "dt = 1/fps\n",
        "# Control input (u)\n",
        "u_x = 0.\n",
        "u_y = 0.\n",
        "# Process variance (Q)\n",
        "var_w =\n",
        "# Measurement variance (R)\n",
        "varz_x =\n",
        "varz_y =\n",
        "# initial variance (P)\n",
        "initial_var_vel = 400.\n",
        "initial_var_pos = 400.\n",
        "\n",
        "KF = KalmanFilter(dt, u_x, u_y, var_w, varz_x, varz_y, initial_var_pos, initial_var_vel)"
      ],
      "metadata": {
        "id": "UxQXr48VKTr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the implementation, rerun from this point to test your code. Don't forget to put the name of your video in the variable `video_name_result`."
      ],
      "metadata": {
        "id": "OfkvWYwqagz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Folders to store videos and images\n",
        "video_path = '/content/drive/MyDrive/Colab Notebooks/KALMAN_2024/Video Datasets'\n",
        "image_path = '/content/drive/MyDrive/Colab Notebooks/KALMAN_2024/Image Datasets'\n",
        "# Subfolders of 'Image Datasets' used to store the images:\n",
        "\n",
        "sub_data2 = 'Result_Filter'\n",
        "video_name_result = 'BallTrackingResult_Filter.mp4'\n",
        "# Folders to store images and videos\n",
        "path_filter = os.path.join(image_path, sub_data2)\n",
        "imgs = os.listdir(path_filter)\n",
        "for img in imgs:\n",
        "    os.remove(f'{path_filter}/{img}')\n",
        "try:\n",
        "    os.remove(f'{video_path}/{video_name_result}')\n",
        "except:\n",
        "    print(\"Not such file\", video_name_result)\n"
      ],
      "metadata": {
        "id": "2HyQNzMRpfVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Video\n",
        "webcam = cv2.VideoCapture(video_file)\n",
        "(_, frame) = webcam.read()\n",
        "cv2_imshow(frame)"
      ],
      "metadata": {
        "id": "8DHAXzBgP5gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Response\n",
        "N = total\n",
        "\n",
        "count = 0\n",
        "while count < N-1:\n",
        "    if not webcam.isOpened():\n",
        "        print('Unable to load the video file.')\n",
        "        sleep(5)\n",
        "        pass\n",
        "    (_, frame) = webcam.read()\n",
        "    #detect_ball(frame)\n",
        "    [x,y,radius, Flag] = detect_ball(frame, HSV_lower_yellow, HSV_upper_yellow)\n",
        "    z = [x,y]\n",
        "\n",
        "    if (count != 0 ):\n",
        "###############\n",
        "## YOUR CODE ##\n",
        "###############\n",
        "\n",
        "    count += 1\n",
        "webcam.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "OwMs4z88toIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = [img for img in os.listdir(path_filter) if img.endswith(\".png\")]\n",
        "\n",
        "frame = cv2.imread(os.path.join(path_filter, images[0]))\n",
        "height, width, layers = frame.shape\n",
        "\n",
        "video_name_result = 'BallTrackingResult_Filter.mp4'\n",
        "video_path_result = os.path.join(video_path , video_name_result)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "video = cv2.VideoWriter(video_path_result, fourcc, fps/3, (width,height))\n",
        "\n",
        "for image in images:\n",
        "    frame = cv2.imread(os.path.join(path_filter, image))\n",
        "    video.write(frame)\n",
        "\n",
        "video.release()\n",
        "print(\"Video generated successfully.\")"
      ],
      "metadata": {
        "id": "ThdTuo6iQRug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result\n",
        "\n",
        "The code below is not working for the video `BallTrackingResult_Filter.mp4`.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1Rjoq7CKl6EE46Ec0xef13ko5n7WGilMX' width=\"250\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "rFoSu_g1ckqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp4 = open(video_path_result,'rb').read()\n",
        "decoded_vid = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f'<video width=400 controls><source src={decoded_vid} type=\"video/mp4\"></video>')"
      ],
      "metadata": {
        "id": "SH1rDGclRu5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font face=\"Verdana\" size=5 color='#40E0D0'> Part III: delivery 3#3\n",
        "\n",
        "Satisfied with your result???? Then, apply the filter in both videos.\n",
        "* Submit a PDF file with Kalman class, the parameters and a brief explanation about your choice, and the new code with the loop in the video frames for tracking.\n",
        "* Submit both videos with Kalman filter.\n",
        "* Compare the results with and without filter, in the ramp and bouncing ball problems, and write your conclusions. Don't forget to discuss the filter parameters.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "59pDFSiutq3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"Verdana\" size=6 color='#6495ED'>  Thank you for actively engaging in the Kalman Filter classes! We trust that you found the content enjoyable and enriching.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1fV0ruX4TCznLnNw-3ZV390p2JyU_-llu' width=\"600\"></center>\n",
        "\n",
        "Do you have any suggestions to enhance our classes? Feel free to share your thoughts by sending us a message; we would greatly appreciate and value your input."
      ],
      "metadata": {
        "id": "Jjxa5t5yF8B4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}